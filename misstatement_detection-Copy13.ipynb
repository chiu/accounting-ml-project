{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "spark = SparkSession.builder.appName('733').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annual_df = spark.read.csv('../annual_compustat.csv', header=True, inferSchema=True).limit(1000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nullcounts = spark.read.csv('annual_compustat_null_count.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('annual_compustat_null_count.csv', 'r') as f:\n",
    "  reader = csv.reader(f)\n",
    "  your_list = list(reader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_count_list = your_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_count_list = [float(x) for x in null_count_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_columns = []\n",
    "for i in range(0, len(null_count_list)):\n",
    "    if null_count_list[i]==0:\n",
    "        good_columns.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "great_columns = [annual_df.columns[i] for i in good_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "great_columns.append('rea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_num = [3, 10, 14]\n",
    "annual_df = annual_df.select(*great_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_dict = {}\n",
    "for x in annual_df.columns:\n",
    "    some_dict[x] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permuted_annual_df = annual_df.fillna(some_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permuted_annual_dtypes = permuted_annual_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_string_columns = [k for (k,v) in permuted_annual_dtypes if v != 'string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permuted_annual_df_no_strings = permuted_annual_df.select(*non_string_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_columns = [item for item in permuted_annual_df_no_strings.columns if item not in ['rea', 'features']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "final_df = assembler.transform(permuted_annual_df_no_strings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_final_df = final_df.drop(*feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|   rea|            features|\n",
      "+------+--------------------+\n",
      "|   0.0|[1000.0,1.9611231...|\n",
      "|   0.0|[1000.0,1.9621231...|\n",
      "|   0.0|[1000.0,1.9631231...|\n",
      "|   0.0|[1000.0,1.9641231...|\n",
      "|   0.0|[1000.0,1.9651231...|\n",
      "|   0.0|[1000.0,1.9661231...|\n",
      "|   0.0|[1000.0,1.9671231...|\n",
      "|   0.0|[1000.0,1.9681231...|\n",
      "| 2.772|[1000.0,1.9691231...|\n",
      "|   0.0|[1000.0,1.9701231...|\n",
      "|   0.0|[1000.0,1.9711231...|\n",
      "|   0.0|[1000.0,1.9721231...|\n",
      "|   0.0|[1000.0,1.9731231...|\n",
      "|   0.0|[1000.0,1.9741231...|\n",
      "|-1.656|[1000.0,1.9751231...|\n",
      "|   0.0|[1000.0,1.9761231...|\n",
      "|   0.0|[1000.0,1.9771231...|\n",
      "|   0.0|[1001.0,1.9781231...|\n",
      "|   0.0|[1001.0,1.9791231...|\n",
      "|   0.0|[1001.0,1.9801231...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_final_df = final_final_df.withColumn('label', final_final_df.rea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------+\n",
      "|   rea|            features| label|\n",
      "+------+--------------------+------+\n",
      "|   0.0|[1000.0,1.9611231...|   0.0|\n",
      "|   0.0|[1000.0,1.9621231...|   0.0|\n",
      "|   0.0|[1000.0,1.9631231...|   0.0|\n",
      "|   0.0|[1000.0,1.9641231...|   0.0|\n",
      "|   0.0|[1000.0,1.9651231...|   0.0|\n",
      "|   0.0|[1000.0,1.9661231...|   0.0|\n",
      "|   0.0|[1000.0,1.9671231...|   0.0|\n",
      "|   0.0|[1000.0,1.9681231...|   0.0|\n",
      "| 2.772|[1000.0,1.9691231...| 2.772|\n",
      "|   0.0|[1000.0,1.9701231...|   0.0|\n",
      "|   0.0|[1000.0,1.9711231...|   0.0|\n",
      "|   0.0|[1000.0,1.9721231...|   0.0|\n",
      "|   0.0|[1000.0,1.9731231...|   0.0|\n",
      "|   0.0|[1000.0,1.9741231...|   0.0|\n",
      "|-1.656|[1000.0,1.9751231...|-1.656|\n",
      "|   0.0|[1000.0,1.9761231...|   0.0|\n",
      "|   0.0|[1000.0,1.9771231...|   0.0|\n",
      "|   0.0|[1001.0,1.9781231...|   0.0|\n",
      "|   0.0|[1001.0,1.9791231...|   0.0|\n",
      "|   0.0|[1001.0,1.9801231...|   0.0|\n",
      "+------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final_final_df.write.parquet(\"final_final_df2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "ml_df = sqlContext.read.parquet(\"final_final_df2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------+\n",
      "|   rea|            features| label|\n",
      "+------+--------------------+------+\n",
      "|   0.0|[1000.0,1.9611231...|   0.0|\n",
      "|   0.0|[1000.0,1.9621231...|   0.0|\n",
      "|   0.0|[1000.0,1.9631231...|   0.0|\n",
      "|   0.0|[1000.0,1.9641231...|   0.0|\n",
      "|   0.0|[1000.0,1.9651231...|   0.0|\n",
      "|   0.0|[1000.0,1.9661231...|   0.0|\n",
      "|   0.0|[1000.0,1.9671231...|   0.0|\n",
      "|   0.0|[1000.0,1.9681231...|   0.0|\n",
      "| 2.772|[1000.0,1.9691231...| 2.772|\n",
      "|   0.0|[1000.0,1.9701231...|   0.0|\n",
      "|   0.0|[1000.0,1.9711231...|   0.0|\n",
      "|   0.0|[1000.0,1.9721231...|   0.0|\n",
      "|   0.0|[1000.0,1.9731231...|   0.0|\n",
      "|   0.0|[1000.0,1.9741231...|   0.0|\n",
      "|-1.656|[1000.0,1.9751231...|-1.656|\n",
      "|   0.0|[1000.0,1.9761231...|   0.0|\n",
      "|   0.0|[1000.0,1.9771231...|   0.0|\n",
      "|   0.0|[1001.0,1.9781231...|   0.0|\n",
      "|   0.0|[1001.0,1.9791231...|   0.0|\n",
      "|   0.0|[1001.0,1.9801231...|   0.0|\n",
      "+------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,-1.06449397447e-06,-0.00244103246633,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "Intercept: 25.74190069013414\n",
      "numIterations: 11\n",
      "objectiveHistory: [0.5000000000000001, 0.4999142911765882, 0.4998611581179943, 0.4998610757468472, 0.49986107450295986, 0.4998610630318895, 0.4998610567796463, 0.499861007730838, 0.49986088950678204, 0.4998608886775636, 0.4998608885466327]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|-0.07899879220746797|\n",
      "|-0.06591281999643073|\n",
      "|-0.05282684778539348|\n",
      "|-0.03974087557435624|\n",
      "|-0.02665490336331...|\n",
      "|-0.01356893115227...|\n",
      "|-4.82958941244504...|\n",
      "| 0.01260301326979274|\n",
      "|    2.79768898548083|\n",
      "| 0.03877495769186723|\n",
      "| 0.05186092990290447|\n",
      "| 0.06494690211394172|\n",
      "| 0.07803287432498252|\n",
      "| 0.09111884653601621|\n",
      "| -1.5517951812529465|\n",
      "|  0.1172907909580907|\n",
      "| 0.13037676316912794|\n",
      "| 0.14346273538016519|\n",
      "| 0.15654870759120243|\n",
      "| 0.16963467980224323|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 10.106638\n",
      "r2: 0.001070\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "train = final_final_df\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------+\n",
      "|   rea|            features| label|\n",
      "+------+--------------------+------+\n",
      "|   0.0|[1000.0,1.9611231...|   0.0|\n",
      "|   0.0|[1000.0,1.9621231...|   0.0|\n",
      "|   0.0|[1000.0,1.9631231...|   0.0|\n",
      "|   0.0|[1000.0,1.9641231...|   0.0|\n",
      "|   0.0|[1000.0,1.9651231...|   0.0|\n",
      "|   0.0|[1000.0,1.9661231...|   0.0|\n",
      "|   0.0|[1000.0,1.9671231...|   0.0|\n",
      "|   0.0|[1000.0,1.9681231...|   0.0|\n",
      "| 2.772|[1000.0,1.9691231...| 2.772|\n",
      "|   0.0|[1000.0,1.9701231...|   0.0|\n",
      "|   0.0|[1000.0,1.9711231...|   0.0|\n",
      "|   0.0|[1000.0,1.9721231...|   0.0|\n",
      "|   0.0|[1000.0,1.9731231...|   0.0|\n",
      "|   0.0|[1000.0,1.9741231...|   0.0|\n",
      "|-1.656|[1000.0,1.9751231...|-1.656|\n",
      "|   0.0|[1000.0,1.9761231...|   0.0|\n",
      "|   0.0|[1000.0,1.9771231...|   0.0|\n",
      "|   0.0|[1001.0,1.9781231...|   0.0|\n",
      "|   0.0|[1001.0,1.9791231...|   0.0|\n",
      "|   0.0|[1001.0,1.9801231...|   0.0|\n",
      "+------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_df = ml_df.withColumn('boolean_label', ml_df.rea != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_df = ml_df.withColumn('label', ml_df.boolean_label.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----+-------------+\n",
      "|   rea|            features|label|boolean_label|\n",
      "+------+--------------------+-----+-------------+\n",
      "|   0.0|[1000.0,1.9611231...|  0.0|        false|\n",
      "|   0.0|[1000.0,1.9621231...|  0.0|        false|\n",
      "|   0.0|[1000.0,1.9631231...|  0.0|        false|\n",
      "|   0.0|[1000.0,1.9641231...|  0.0|        false|\n",
      "|   0.0|[1000.0,1.9651231...|  0.0|        false|\n",
      "|   0.0|[1000.0,1.9661231...|  0.0|        false|\n",
      "|   0.0|[1000.0,1.9671231...|  0.0|        false|\n",
      "|   0.0|[1000.0,1.9681231...|  0.0|        false|\n",
      "| 2.772|[1000.0,1.9691231...|  1.0|         true|\n",
      "|   0.0|[1000.0,1.9701231...|  0.0|        false|\n",
      "|   0.0|[1000.0,1.9711231...|  0.0|        false|\n",
      "|   0.0|[1000.0,1.9721231...|  0.0|        false|\n",
      "|   0.0|[1000.0,1.9731231...|  0.0|        false|\n",
      "|   0.0|[1000.0,1.9741231...|  0.0|        false|\n",
      "|-1.656|[1000.0,1.9751231...|  1.0|         true|\n",
      "|   0.0|[1000.0,1.9761231...|  0.0|        false|\n",
      "|   0.0|[1000.0,1.9771231...|  0.0|        false|\n",
      "|   0.0|[1001.0,1.9781231...|  0.0|        false|\n",
      "|   0.0|[1001.0,1.9791231...|  0.0|        false|\n",
      "|   0.0|[1001.0,1.9801231...|  0.0|        false|\n",
      "+------+--------------------+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_df = ml_df.drop('rea').drop('boolean_label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "splits = ml_df.randomSplit([0.6, 0.4], 12)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 3 (classes)\n",
    "layers = [1514, 1514, 1514, 2]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=10, layers=layers, blockSize=128, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# model = trainer.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # compute accuracy on the test set\n",
    "# result = model.transform(test)\n",
    "# predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "# evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "# print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label_np = np.array(train.select('label').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_np = np.array(train.select('features').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_np_flat = [x[0] for x in features_np]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = np.vstack(features_np_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a single-input model with 2 classes (binary classification):\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Dense(18, activation='relu', input_dim=9))\n",
    "# model.add(Dense(18, activation='relu', input_dim=18))\n",
    "model.add(Dense(1, input_dim = 18,  activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "# import numpy as np\n",
    "# data = np.random.random((1000, 100))\n",
    "# labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(result, label_np, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(label_np, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "547/(49+547)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
